{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61f3a4e-2774-4885-a9e5-4bf17b0204ad",
   "metadata": {},
   "source": [
    "# COSE474-2024F: Deep Learning HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab48a1-5dd2-4205-b9e0-a2c01d8e480a",
   "metadata": {},
   "source": [
    "## 7.1 From Fully Connected Layers to Convolutions"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.1.1 Invariance",
   "id": "2a825be20ee921e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The position of the patch of the image that is currently analyzed should not be treated differently depending on where the patch is located in the image. (translation invariance) Depending on how deep a layer is in the network it should be able to capture longer-range features.",
   "id": "e8d7ee9ff17c1263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b4ebb1cc51b75aba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.1.4 Channels",
   "id": "76a09b453c91438e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One pixel in an image has 3 channels (R - Red, G - Green, B - Blue) Therefore it is also a good idea to incorporate multiple channels into the hidden representations of our input. These channels are also called feature maps.",
   "id": "dcf683aee0a93eb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.2 Convolutions for Images",
   "id": "fd64089614bd7031"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2.1 The Cross-Correlation Operation",
   "id": "6f891b1623d9977d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.061701Z",
     "start_time": "2024-10-09T09:23:15.151589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "id": "89723f1be03716aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteo/PycharmProjects/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.066355Z",
     "start_time": "2024-10-09T09:23:22.063082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ],
   "id": "e22a8edf7c297ea6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.075974Z",
     "start_time": "2024-10-09T09:23:22.067194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ],
   "id": "43f1cf3f2bee11b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2.2 Convolutional Layers",
   "id": "9f1a9ae586a22750"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.080525Z",
     "start_time": "2024-10-09T09:23:22.077670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ],
   "id": "2809e101e6bc9887",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2.3 Object Edge Detection in Images",
   "id": "77280c24b3fefd10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.085143Z",
     "start_time": "2024-10-09T09:23:22.081850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ],
   "id": "516cb15afd6ad143",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.087959Z",
     "start_time": "2024-10-09T09:23:22.085885Z"
    }
   },
   "cell_type": "code",
   "source": "K = torch.tensor([[1.0, -1.0]])",
   "id": "e6103d447df447d0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.091785Z",
     "start_time": "2024-10-09T09:23:22.088773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ],
   "id": "d5a04a809ccf5a10",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.095569Z",
     "start_time": "2024-10-09T09:23:22.092435Z"
    }
   },
   "cell_type": "code",
   "source": "corr2d(X.t(), K)",
   "id": "8406e9c74a77fb76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2.4 Learning a Kernel",
   "id": "ea49eb52ddc1cf66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.168980Z",
     "start_time": "2024-10-09T09:23:22.096557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2 \n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
   ],
   "id": "6c00da22213e8fd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 8.982\n",
      "epoch 4, loss 1.508\n",
      "epoch 6, loss 0.253\n",
      "epoch 8, loss 0.043\n",
      "epoch 10, loss 0.007\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.175975Z",
     "start_time": "2024-10-09T09:23:22.172272Z"
    }
   },
   "cell_type": "code",
   "source": "conv2d.weight.data.reshape((1, 2))",
   "id": "7b8486678a884fde",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9852, -0.9836]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2.5 Cross-Correlation and Convolution",
   "id": "87b88bd7972f672d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In order to receive the output of a strict convolution operation from a two-dimensional convolution layer instead of a cross-correlation, the kernel tensor needs to be flipped both horizontally and vertically before performing the cross-correlation operation with the input tensor. The output of the convolution layers does not change depending on if a convolution or cross-correlation operation was performed.",
   "id": "8d3dab0a4fd8ab95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2.6 Feature Map and Receptive Field",
   "id": "7564e9753f8fc485"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Feature Map:** \n",
    "The output from a convolutional layer. Learned representations of the dimensions to the next layer.\n",
    "\n",
    "**Receptive Field:**\n",
    "For any element $x$ in a CNN, the receptive field refers to any other element in the CNN that has an influence on the calculation of $x$ during forward propagation."
   ],
   "id": "4653ee788b9591d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.3 Padding and Stride",
   "id": "373817d788d1e7b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.178963Z",
     "start_time": "2024-10-09T09:23:22.176750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn"
   ],
   "id": "c59992b4a4307ab4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.3.1 Padding",
   "id": "b113c4ed974da7ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One problem when applying convolutional layers, is that information of pixels at the edges often get lost. This can get mitigated by using a smaller kernel tensor, but after applying multiple convolutional layers, this effect can stack up. To solve this problem padding is added to the input tensor, by adding pixels with value 0 to the edges of the input (padding). The amount of padding depends on the size of the kernel, with the height of the kernel - 1 being added on top and bottom and the width of the kernel - 1 being added to either side.",
   "id": "9f758991c67ac59c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.183845Z",
     "start_time": "2024-10-09T09:23:22.179567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ],
   "id": "5201df90c6287ded",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.188176Z",
     "start_time": "2024-10-09T09:23:22.184426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ],
   "id": "e6e5988a81ad761f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.3.2 Stride",
   "id": "faab4073150da833"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To compute the cross-correlation, the convolution window is slided across the input from the top left of the input. The stride defines how many rows and columns the window gets slided. This can be useful to optimize performance or for down-scaling the output of the convolution layer. ",
   "id": "484776b5c2eb6a66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.192166Z",
     "start_time": "2024-10-09T09:23:22.188904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ],
   "id": "e93489dc669c6e49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.197106Z",
     "start_time": "2024-10-09T09:23:22.193109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ],
   "id": "1d70aff874031dde",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.4 Multiple Input and Multiple Output Channels",
   "id": "9a0f218382e9ce8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.200049Z",
     "start_time": "2024-10-09T09:23:22.198065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ],
   "id": "7a44c886ce774a43",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.4.1 Multiple Input Channels",
   "id": "69685dbbc6744302"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If our input has multiple channels, the input tensor will be a third-degree tensor. Therefore, our kernel needs to have the same amount of channels as the input tensor and also be a third-degree tensor. To compute the output of the convolution layer, the cross-correlation is calculated for every channel of the input with the corresponding channel of the kernel. The results are then added together, yielding a second-degree tensor as the output of the convolution channel. ",
   "id": "d2524b82b13303f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.203193Z",
     "start_time": "2024-10-09T09:23:22.200807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
   ],
   "id": "d9d9468af4841fd2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.209059Z",
     "start_time": "2024-10-09T09:23:22.204034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ],
   "id": "c33e60155ce509d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.4.2 Multiple Output Channels",
   "id": "3d11db2d9f51c7bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Having multiple channels in every layer is very important. If we want to have $c_o$ output channels from $c_i$ input channels, we need to construct $c_o$ third-degree kernels with $c_i$ channels each, creating a fourth-degree tensor as a kernel. The output is then a third-degree tensor with $c_o$ channels.",
   "id": "52195c5094a234a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.212117Z",
     "start_time": "2024-10-09T09:23:22.209956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ],
   "id": "72804a47feaa7cbd",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.215708Z",
     "start_time": "2024-10-09T09:23:22.212878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "K.shape"
   ],
   "id": "c50de178affde449",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.219993Z",
     "start_time": "2024-10-09T09:23:22.216498Z"
    }
   },
   "cell_type": "code",
   "source": "corr2d_multi_in_out(X, K)",
   "id": "4d72539ee0f190d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.4.3 $1\\times1$ Convolutional Layer",
   "id": "71d501590a25b440"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "While a $1\\times1$ convolution window is not able to pick up on patterns across multiple adjacent pixels, it is useful when trying to find patterns between the different channels. An input with $c_i$ channels is transformed into an output with $c_o$ channels while preserving its size.",
   "id": "c00caf1cde5b6712"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.223915Z",
     "start_time": "2024-10-09T09:23:22.221476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))"
   ],
   "id": "ef40aad3233d3ade",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.228987Z",
     "start_time": "2024-10-09T09:23:22.224725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
   ],
   "id": "3a4c1da4dc70453e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.5 Pooling",
   "id": "df627d756be32504"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.231726Z",
     "start_time": "2024-10-09T09:23:22.229900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "id": "dafb75cfec156a18",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.5.1 Maximum Pooling and Average Pooling",
   "id": "701d16030b0f0ee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Similar to Convolution, in Pooling a window is slid over the input. However, the Pooling layer does not contain any parameters and calculates a deterministic output, often the maximum (Maximum Pooling) or average (Average Pooling) in the pooling window, to reduce the size of the output.",
   "id": "4e633288dacf26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.235505Z",
     "start_time": "2024-10-09T09:23:22.232555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ],
   "id": "6356eff610aa8764",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.240668Z",
     "start_time": "2024-10-09T09:23:22.236237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))"
   ],
   "id": "74c24a7ba01ae4d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.244952Z",
     "start_time": "2024-10-09T09:23:22.241901Z"
    }
   },
   "cell_type": "code",
   "source": "pool2d(X, (2, 2), 'avg')",
   "id": "8e149621b2e2efe9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.5.2 Padding and Stride",
   "id": "79b9e354cbce5b51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As with convolution layers, the padding of the input and the stride of the window can be adjusted in Pooling layers. For Pooling the function ```nn.MaxPool2d()``` can be used to perform Max-Pooling.",
   "id": "d725d2356d5d982e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.254156Z",
     "start_time": "2024-10-09T09:23:22.250030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ],
   "id": "64e120b4ed822e83",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.258068Z",
     "start_time": "2024-10-09T09:23:22.255075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pool2d = nn.MaxPool2d(3)\n",
    "pool2d(X)"
   ],
   "id": "216e642beb953aec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.262222Z",
     "start_time": "2024-10-09T09:23:22.258994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ],
   "id": "c44f9a6c4d995411",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.266607Z",
     "start_time": "2024-10-09T09:23:22.263036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
    "pool2d(X)"
   ],
   "id": "ee55dd9eab7fa7bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.5.3 Multiple Channels",
   "id": "9fe129eb52c03347"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When pooling over multiple channels, the output of each channel is not aggregated with the other channels. This means that the output has just as many channels as the input tensor.",
   "id": "b5c499de87faa1ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.271129Z",
     "start_time": "2024-10-09T09:23:22.267618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.cat((X, X + 1), 1)\n",
    "X"
   ],
   "id": "56b96e3c4b2a1e05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.274932Z",
     "start_time": "2024-10-09T09:23:22.271927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ],
   "id": "ec92babe00f4ff21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.6 Convolutional Neural Networks (LeNet)",
   "id": "4bd3b56f881b351d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.277876Z",
     "start_time": "2024-10-09T09:23:22.276073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "id": "ba07f45a9c481efc",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.6.1 LeNet",
   "id": "67b5cb2e4f6c5b59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The LeNet consists of two parts: \n",
    "1) Convolutional encoder consisting of two convolutional layers\n",
    "2) Dense Block of three fully connected layers"
   ],
   "id": "b3bd5c1a6f9f7bd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.282663Z",
     "start_time": "2024-10-09T09:23:22.279251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_cnn(module):  \n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "class LeNet(d2l.Classifier):  \n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ],
   "id": "6fd7094cfb9d5229",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:23:22.290400Z",
     "start_time": "2024-10-09T09:23:22.283755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@d2l.add_to_class(d2l.Classifier)  #@save\n",
    "def layer_summary(self, X_shape):\n",
    "    X = torch.randn(*X_shape)\n",
    "    for layer in self.net:\n",
    "        X = layer(X)\n",
    "        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "\n",
    "model = LeNet()\n",
    "model.layer_summary((1, 1, 28, 28))"
   ],
   "id": "9295273315b4616c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape:\t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape:\t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape:\t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape:\t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape:\t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 400])\n",
      "Linear output shape:\t torch.Size([1, 120])\n",
      "Sigmoid output shape:\t torch.Size([1, 120])\n",
      "Linear output shape:\t torch.Size([1, 84])\n",
      "Sigmoid output shape:\t torch.Size([1, 84])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.6.2 Training",
   "id": "8c4ce1d8df69db6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After implementing the model, we now inspect its performance on the Fashion-MNIST",
   "id": "21ed9e8eeebdfc56"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model = LeNet(lr=0.1)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
    "trainer.fit(model, data)"
   ],
   "id": "e5e219dbd1d3e91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8.2 Networks Using Blocks (VGG)",
   "id": "8a21bd6edfe31e74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:25:24.916775Z",
     "start_time": "2024-10-09T09:25:24.913555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "id": "232674ea4e590c97",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.2.1 VGG Blocks",
   "id": "766e6f5417930766"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A VGG Block consists of a sequence of convolutions with $3\\times3$ kernels and padding of $1$, followed by a $2\\times2$ Max-Pooling Layer with a stride of $2$, shrinking the size of the input by half after each block.",
   "id": "34fc4159f237bafb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:25:24.921521Z",
     "start_time": "2024-10-09T09:25:24.918170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vgg_block(num_convs, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)"
   ],
   "id": "128c30ff16373fa3",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.2.2 VGG Network",
   "id": "c48298617143614c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The VGG Network can be split into two parts:\n",
    "1) Several VGG blocks\n",
    "2) Fully connected layers identical to AlexNet"
   ],
   "id": "740ee7430c9e0c3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:25:24.926493Z",
     "start_time": "2024-10-09T09:25:24.923038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VGG(d2l.Classifier):\n",
    "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        conv_blks = []\n",
    "        for (num_convs, out_channels) in arch:\n",
    "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
    "        self.net = nn.Sequential(\n",
    "            *conv_blks, nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        self.net.apply(d2l.init_cnn)"
   ],
   "id": "bb389cb3ccd1e23f",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:25:25.380147Z",
     "start_time": "2024-10-09T09:25:24.928095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n",
    "    (1, 1, 224, 224))"
   ],
   "id": "1c10b9d77d8f8992",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.2.3 Training",
   "id": "856acab17cfe7553"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The VGG-Network is now trained on Fashion-MNIST.",
   "id": "278d1ad05f5f2e94"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-09T09:25:25.381041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ],
   "id": "403192ff4d047e02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8.6 Residual Networks (ResNet) and ResNeXt",
   "id": "2e7a5f34704a400b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ],
   "id": "f55180a002599f2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.6.1 Function Classes",
   "id": "b58d2219ef02764a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e0857c27f4cc566d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.6.2 Residual Blocks",
   "id": "5623f83e51a80195"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instead of having to learn the underlying mapping $f(x)$ directly as a regular block would, a residual block has to learn the residual mapping $g(x)=f(x)-x$. If the underlying mapping is the identity function $f(x)=x$, using a residual block makes this a lot easier. With residual blocks, inputs can forward propagate faster through the residual connections across layers.",
   "id": "eacadd6b91a0633d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class Residual(nn.Module):  #@save\n",
    "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ],
   "id": "4af1948a7c12d8d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "blk = Residual(3)\n",
    "X = torch.randn(4, 3, 6, 6)\n",
    "blk(X).shape"
   ],
   "id": "7d76d2ee24d0d9b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "blk = Residual(3)\n",
    "X = torch.randn(4, 3, 6, 6)\n",
    "blk(X).shape"
   ],
   "id": "39cdf248354fa41f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.6.3 ResNet Model",
   "id": "bdebcbd3ae35d0bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Following Code describes the Implementation of the ResNet.",
   "id": "76115a74d9e23bcf"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNet(d2l.Classifier):\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ],
   "id": "82fba7c35bb56cc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "@d2l.add_to_class(ResNet)\n",
    "def block(self, num_residuals, num_channels, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels))\n",
    "    return nn.Sequential(*blk)"
   ],
   "id": "c2462c281361689c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "@d2l.add_to_class(ResNet)\n",
    "def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = nn.Sequential(self.b1())\n",
    "    for i, b in enumerate(arch):\n",
    "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
    "    self.net.add_module('last', nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "        nn.LazyLinear(num_classes)))\n",
    "    self.net.apply(d2l.init_cnn)"
   ],
   "id": "ad91c569f4c8afaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNet18(ResNet):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
    "                       lr, num_classes)\n",
    "\n",
    "ResNet18().layer_summary((1, 1, 96, 96))"
   ],
   "id": "61def40158ef4b5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.6.4 Training",
   "id": "d4b155d9cac419a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Model is now trained on the Fashion-MNIST.",
   "id": "a43bba6610134329"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model = ResNet18(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ],
   "id": "e0d617929e89bc77",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNnrZE7IYqb9Zg9/abOKw6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteomrz/20242R0136COSE47402/blob/main/final/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mwhNL4nQgNx",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install openai-clip\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "ds = load_dataset(\"bazyl/GTSRB\")\n",
        "\n",
        "train_full = ds['train']\n",
        "test_full = ds['test']\n",
        "\n",
        "# Base label for text description\n",
        "base_label = 'a picture of a street sign warning about '\n",
        "\n",
        "# Map used Street Sign IDs to text descriptions\n",
        "id_to_description = {\n",
        "    18: \"General caution\",\n",
        "    19: \"Dangerous curve left\",\n",
        "    20: \"Dangerous curve right\",\n",
        "    21: \"Winding road\",\n",
        "    22: \"Bumpy road\",\n",
        "    23: \"Slippery road\",\n",
        "    24: \"Road narrows on the right\",\n",
        "    25: \"Road work\",\n",
        "    26: \"Traffic lights\",\n",
        "    27: \"Pedestrians\",\n",
        "    28: \"Children crossing\",\n",
        "    29: \"Bike crossing\",\n",
        "    30: \"Beware of ice/snow\",\n",
        "    31: \"Wild animals crossing\",\n",
        "}\n",
        "\n",
        "# Filter for warning signs\n",
        "train_full = [example for example in train_full if example['ClassId'] in id_to_description]\n",
        "test_full = [example for example in test_full if example['ClassId'] in id_to_description]\n",
        "\n",
        "# Add Text Description\n",
        "for instance in train_full:\n",
        "    instance['Description'] = base_label + id_to_description[instance['ClassId']]\n",
        "\n",
        "len_train = int(0.8 * len(train_full))\n",
        "train, val = random_split(train_full, [len_train, len(train_full) - len_train])"
      ],
      "metadata": {
        "id": "sh7PWl3KQmcQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "-lujhaRVXEKG",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WarningSignDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image = Image.open(BytesIO(item['Path']['bytes']))\n",
        "        return preprocess(image), item['Description'], item['ClassId'] - 18"
      ],
      "metadata": {
        "id": "6JOMEw9huWfC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False)\n",
        "\n",
        "train_dataloader = DataLoader(WarningSignDataset(train),batch_size = 32, shuffle=True) #Define your own dataloader\n",
        "val_dataloader = DataLoader(WarningSignDataset(val),batch_size = 32, shuffle=False) #Define your own dataloader\n",
        "\n",
        "# without this the loss is NaN\n",
        "#https://github.com/openai/CLIP/issues/57\n",
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()"
      ],
      "metadata": {
        "id": "-LtyviOTFIYJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "params = model.parameters()\n",
        "optimizer = optim.Adam(params, lr=1e-3) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
        "\n",
        "# add your own code to track the training progress.\n",
        "for epoch in range(8):\n",
        "  running_loss = 0.0\n",
        "  pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{8}, Loss: 0.0000\")\n",
        "  for images, texts, _ in pbar :\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      images= images.to(device)\n",
        "      texts = clip.tokenize(texts).to(device)\n",
        "\n",
        "      logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "\n",
        "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "      total_loss.backward()\n",
        "      running_loss += total_loss.item()\n",
        "\n",
        "      # Fixes NaN loss\n",
        "      if device == \"cpu\":\n",
        "         optimizer.step()\n",
        "      else :\n",
        "        convert_models_to_fp32(model)\n",
        "        optimizer.step()\n",
        "        clip.model.convert_weights(model)\n",
        "\n",
        "      pbar.set_description(f\"Epoch {epoch+1}/{8}, Loss: {running_loss/len(train_dataloader):.4f}\")\n",
        "  print(f\"Epoch {epoch+1}/{8}, Loss: {running_loss/len(train_dataloader):.4f}\")\n",
        "\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for images, texts, classId in val_dataloader:\n",
        "      images= images.to(device)\n",
        "      texts = clip.tokenize(texts).to(device)\n",
        "      logits_per_image, logits_per_text = model(images, texts)\n",
        "      pred = torch.argmax(logits_per_image, dim=-1).cpu().numpy()\n",
        "      total += len(images)\n",
        "      correct += (pred[0] == classId).sum().item()\n",
        "  print(f\"Accuracy: {100*correct/total}%\")\n"
      ],
      "metadata": {
        "id": "lKRm1fHsHUTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
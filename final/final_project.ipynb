{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaBoI8gCxVvtSJ+bUN9RZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteomrz/20242R0136COSE47402/blob/main/final/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "-mwhNL4nQgNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b9cb42-0e50-4cbd-9993-623fc1670967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-clip in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from openai-clip) (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from openai-clip) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-clip) (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-clip\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "ds = load_dataset(\"bazyl/GTSRB\")\n",
        "\n",
        "train = ds['train']\n",
        "test = ds['test']\n",
        "\n",
        "# Base label for text description\n",
        "base_label = 'a picture of a '\n",
        "end_label = ' street sign'\n",
        "\n",
        "# Map used Street Sign IDs to text descriptions\n",
        "id_to_description = {\n",
        "    18: \"General caution\",\n",
        "    19: \"Dangerous curve left\",\n",
        "    20: \"Dangerous curve right\",\n",
        "    21: \"Winding road\",\n",
        "    22: \"Bumpy road\",\n",
        "    23: \"Slippery road\",\n",
        "    24: \"Road narrows on the right\",\n",
        "    25: \"Road work\",\n",
        "    26: \"Traffic lights\",\n",
        "    27: \"Pedestrians\",\n",
        "    28: \"Children crossing\",\n",
        "    29: \"Bike crossing\",\n",
        "    30: \"Beware of ice/snow\",\n",
        "    31: \"Wild animals crossing\",\n",
        "}\n",
        "\n",
        "# Filter for warning signs\n",
        "train = [example for example in train if example['ClassId'] in id_to_description]\n",
        "test = [example for example in test if example['ClassId'] in id_to_description]\n",
        "\n",
        "# Add Text Description\n",
        "for instance in train:\n",
        "    # Prompt tuning\n",
        "    instance['Description'] = base_label + id_to_description[instance['ClassId']] + end_label\n",
        "\n",
        "\n",
        "len_train = int(0.8 * len(train))\n",
        "train, val = random_split(train, [len_train, len(train) - len_train])"
      ],
      "metadata": {
        "id": "sh7PWl3KQmcQ",
        "collapsed": true
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "-lujhaRVXEKG",
        "collapsed": true
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WarningSignDataset(Dataset):\n",
        "    def __init__(self, data, is_test = False):\n",
        "        self.data = data\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image = Image.open(BytesIO(item['Path']['bytes']))\n",
        "        text = item['Description'] if not self.is_test else ''\n",
        "        class_id = item['ClassId'] - min(id_to_description.keys())\n",
        "        return preprocess(image), text, class_id"
      ],
      "metadata": {
        "id": "6JOMEw9huWfC"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False)\n",
        "\n",
        "# Data Loaders for Train, Val and Test\n",
        "train_dataloader = DataLoader(WarningSignDataset(train),batch_size = 32, shuffle=True)\n",
        "val_dataloader = DataLoader(WarningSignDataset(val),batch_size = 32, shuffle=False)\n",
        "test_dataloader = DataLoader(WarningSignDataset(test, is_test=True),batch_size = 32, shuffle=False)\n",
        "\n",
        "# without this the loss is NaN\n",
        "#https://github.com/openai/CLIP/issues/57\n",
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()"
      ],
      "metadata": {
        "id": "-LtyviOTFIYJ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "params = model.parameters()\n",
        "# Try out different optimizers\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=8e-5)\n",
        "\n",
        "epoch_num = 3\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "\n",
        "  # TRAINING\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epoch_num}, Loss: 0.0000\")\n",
        "  for images, texts, _ in pbar :\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      images= images.to(device)\n",
        "      texts = clip.tokenize(texts).to(device)\n",
        "\n",
        "      logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "\n",
        "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "      total_loss.backward()\n",
        "      running_loss += total_loss.item()\n",
        "\n",
        "      # Fixes NaN loss\n",
        "      if device == \"cpu\":\n",
        "         optimizer.step()\n",
        "      else :\n",
        "        convert_models_to_fp32(model)\n",
        "        optimizer.step()\n",
        "        clip.model.convert_weights(model)\n",
        "\n",
        "      pbar.set_description(f\"Epoch {epoch+1}/{epoch_num}, Loss: {running_loss/len(train_dataloader):.4f}\")\n",
        "  print(f\"Epoch {epoch+1}/{epoch_num}, Loss: {running_loss/len(train_dataloader):.4f}\")\n",
        "\n",
        "  # VALIDATION\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for images, texts, classId in val_dataloader:\n",
        "      images= images.to(device)\n",
        "      texts = clip.tokenize(texts).to(device)\n",
        "      image_features = model.encode_image(images)\n",
        "      text_features = model.encode_text(texts)\n",
        "      logits_per_image, logits_per_text = model(images, texts)\n",
        "      pred = torch.argmax(logits_per_image, dim=-1).cpu().numpy()\n",
        "      total += len(images)\n",
        "      for i in range(len(images)):\n",
        "        if classId[i] == classId[pred[i]]:\n",
        "          correct += 1\n",
        "\n",
        "  print(f\"Validation Accuracy: {100*correct/total}%\")\n",
        "\n",
        "torch.save(model.state_dict(), 'clip_finetuned.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKRm1fHsHUTY",
        "outputId": "dd829fd8-9bb7-47ea-847f-3fbb015a2813"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3, Loss: 1.4175: 100%|██████████| 192/192 [00:45<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 1.4175\n",
            "Validation Accuracy: 99.34640522875817%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3, Loss: 1.2549: 100%|██████████| 192/192 [00:45<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 1.2549\n",
            "Validation Accuracy: 99.73856209150327%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3, Loss: 1.2403: 100%|██████████| 192/192 [00:44<00:00,  4.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 1.2403\n",
            "Validation Accuracy: 99.80392156862744%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING\n",
        "all_classes = torch.cat([clip.tokenize(base_label + description + end_label) for description in id_to_description.values()])\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for images, _, classId in test_dataloader:\n",
        "    images= images.to(device)\n",
        "    texts = all_classes.to(device)\n",
        "    logits_per_image, logits_per_text = model(images, texts)\n",
        "    pred = torch.argmax(logits_per_image, dim=-1).cpu().numpy()\n",
        "    total += len(images)\n",
        "    correct += (classId == pred).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100*correct/total}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z0yi6XMWQB7",
        "outputId": "06a704b7-cae6-4c24-b59b-a36975187e3c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 21.68776371308017%\n"
          ]
        }
      ]
    }
  ]
}